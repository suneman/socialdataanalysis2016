{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week we'll be getting into the principles of machine learning, while starting to get to know the dataset. The lecture will feature an overview of fundamentals of machine learning (videos by our resident machine learning expert Ole Winter), and we'll (_finally_) start working on getting to know the dataset we'll be crunching over the next few weeks.\n",
    "\n",
    "Don't forget that the overarching goal of the class is to do _predictive policing_ in the style of the tv-show [_NUMB3ERS_](https://www.youtube.com/watch?v=dSOrAQMXTcc) or Minority Report. In order to do that, however, we need to understand the patterns in the crime dataset. That's what we'll be doing today.\n",
    "\n",
    "* Part 1 of today's exercises will be about getting data.\n",
    "* Part 2 is the \"main course\", where we'll be playing around with real data, plotting and counting.\n",
    "* Part 3 is about machine learning, featuring a couple of videos with Ole Winter from DTU Compute and some simple questions from the book.\n",
    "* And finally, in Part 4, we plot a little bit of the detailed GPS data.\n",
    "* In part 5 you get to (optionally) generalize the book's KNN example to the SF dataset.\n",
    "\n",
    "First, a little unofficial video 1 minute greeting from me - then, let's get to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"225\"\n",
       "            src=\"https://www.youtube.com/embed/CEgGxQFIYsM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104c0d990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mini sune greeting\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"CEgGxQFIYsM\",width=400, height=225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1: Getting the dataset (DSFS Chapter 9)\n",
    "\n",
    "There's no data science without data, so we'll start thinking about ways to acquire data. You've already played around with reading/writing text from files and the exercises today focus on CSV files. For other types of data access, I recommend that you skim this chapter so you have a place to come back to when you need to scrape the web or use various APIs.\n",
    "\n",
    "> _Reading_: Skim DSFS Chapter 9 and pay extra attention to the part about CSV files.  \n",
    "  \n",
    "---  \n",
    "\n",
    "> _Exercise_: Reading in crime-data from San Francisco\n",
    ">  \n",
    "> * Head over to the [SF OpenData page](https://data.sfgov.org), click on the link to _Public Safety_ data and then choose _SFPD Incidents_. Figure out how to download the list of incidents containing _every crime registered since January 1st, 2003_ as a CSV file by clicking around the page.\n",
    "> * Once the file has been downloaded you should read it into Python. The file has may column, and today we'll be working with \n",
    ">   - The various categories of crimes\n",
    ">   - Crime over the years\n",
    ">   - What time of day do crimes occur?\n",
    ">   - And do certain crime-types tend to happen in specific neighborhoods?\n",
    ">   - We'll play around with geo-data. \n",
    "> \n",
    ">   All of this can be easily based on the columns in the incident file. Which columns will you need?\n",
    "\n",
    "> * What is the total number of incidents in the CSV file?\n",
    "\n",
    "** An example of how to read a CSV file**\n",
    "There are many ways to read in CSV files - here's how I usually do it\n",
    "\n",
    "```\n",
    "from csv import reader                          # get the csv reader\n",
    "infile = open(\"/path-to-file/file.csv\", 'r')    # open the file for reading\n",
    "\n",
    "for line in reader( infile ):                   # read through the CSV one line at a time\n",
    "    var1, var2, ..., varN = line                # assign the various fields in the line to variables\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Working with data (DSFS Chapter 10)\n",
    "\n",
    "Once again, the book should be thought of as a useful reference, but today we'll be doing our own datascience rather than following the book slavishly. \n",
    "\n",
    "In order to do awesome _predictive policing_ later on in the class, we're going to dissect the SF crime-data quite thoroughly to figure out what has been going on over the last 10 years on the San Francisco crime scene. It's going to be a lot of work ... but after mastering these exercises you guys will have some real data science skills! \n",
    "\n",
    "> _Reading_: Skim DSFS Chapter 10 so you have a sense of what's in there.\n",
    "\n",
    "---\n",
    "> _Exercise_: The types of crime and their popularity over time. The first field we'll dig into is the column \"Category\".\n",
    "> * Create a `set()` of all the categories of crime in the dataset. How many are there? \n",
    "> * Now count the number of occurrences of each category in the dataset. What is the most commonly occurring category of crime? What is the least frequently occurring?\n",
    "> * Create a histogram over crime occurrences. Mine looks like this\n",
    ">   ![Histogram](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/categoryhist.png)\n",
    "> * Now it's time to explore how the crime statistics change over time. To start off easily, let's count the number of crimes per year for the years 2003-2015 (the full years in the database). For that one, I used the `date` column in the CSV file and used `year = int( date.split(\"/\")[-1] )` to extract the year of each crime. What's the average number of crimes per year? \n",
    "> * Police chief Suneman is interested in the temporal development of only a subset of categories, the so-called focus crimes. Those categories are listed below (for convenient copy-paste action). Now create bar-charts displaying the year-by-year development of each of these categories across the years 2003-2015.\n",
    "> \n",
    ">    _Little note_: As with anything in Python there are many ways of doing everything. A nice choice for counting things like these is `pandas` which give you a neat way of handling tables in Python. But I've created everything I needed for today simply using standard dictionaries for keeping track of my counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * My plot looks like this for the 14 focus crimes:\n",
    "    ![Histograms](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/temporal_categories.png)\n",
    "    Comment on at least three interesting trends in your plot. Take a look at [this article](http://www.nytimes.com/2015/05/23/us/high-rents-elbow-latinos-from-san-franciscos-mission-district.html) or [this one](http://www.cjjc.org/publications/maria-poblets-blog/576-heart-of-sf) - are the trends you observe consistent with the message in the articles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we're getting warmed up. The next exercise looks at the focus crimes, and their temporal patterns across the day. It's interesting to know if some crimes tend to happen during the day ... and if others take during the night time. By knowing _when_ certain types of crimes happen, we can more efficiently fight them!\n",
    "\n",
    "> _Exercises_: The types of crime and their popularity across the 24 hours of the day.\n",
    "> \n",
    "> * First, plot a simple histogram of the number of crimes per hour in the dataset. This piece of information is in the _Time_ column of the CSV file, and I simply used `hr = int( time.split(\":\")[0] )` to get this info out. There's a little strange peak at `12:00` - what do you think is going on there? (No need for a serious exploration, just some thoughts on what might be going on).\n",
    "> * Next create a plot of the count of each crime category across the 24 hours of the day. Again, comment on at least three trends in the data. (My plot looks like this)\n",
    "    ![Histograms](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/daily_patterns_categories.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's much more to work on in this dataset (for example, I also explored if certain crimes tend to happen on certain week-days ... [also some interesting patterns there!](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/weekdays.png)), but we have lots to do, so we must rush onwards. \n",
    "\n",
    "The next thing we'll be looking into is how crimes break down across the 10 districts in San Francisco.\n",
    "\n",
    "_Exercises_: The types of crime and how they take place across San Francisco's police districts.\n",
    "\n",
    "* So now we'll be combining information about _PdDistrict_ and _Category_ to explore differences between SF's neighborhoods. First, simply list the names of SF's 10 police districts.\n",
    "* Which has the most crimes? Which has the most focus crimes?\n",
    "* Next, we want to generate a slightly more complicated graphic. I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical. Below I describe how to get that plot going\n",
    "  - First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole. That's simply a normalized version of [this plot](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/categoryhist.png). Let's call it `P(crime)`.\n",
    "  - Next, we calculate that same probability distribution _but for each PD district_, let's call that `P(crime|district)`.\n",
    "  - Now we look at the ratio `P(crime|district)/P(crime)`. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs _more frequently_ within that district. If it's smaller than one, it means that the crime is _rarer within the district in question_ than in the city as a whole.\n",
    "  - For each district plot these ratios for the 14 focus crimes. My plot looks like this\n",
    "    ![Histograms](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/neighborhood_ratios.png)\n",
    "  - Comment on the top crimes in _Tenderloin_, _Mission_, and _Richmond_. Does this fit with the impression you get of these neighborhoods on Wikipedia?\n",
    "  - Even though we only plotted the ratios for our 14 focus crimes, I asked you to calculate the ratios based on all crime categories. Why do you think I wanted to include all crime types in the calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Machine Learning (DSFS Chapter 11)\n",
    "\n",
    "We won't go too deep with machine learning today, but after all it's time to watch some video lectures on the fundamentals of Machine learning. The lectures have been prepared by our very own expert, Ole Winter, whose work focuses on Machine Learning. The lectures + slides have been prepared especially for you guys by Ole, and lovingly edited by yours truly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.youtube.com/embed/SsCYF9tDY9Y\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104c0d8d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole Winter, \"What is Machine Learning\" \n",
    "YouTubeVideo(\"SsCYF9tDY9Y\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.youtube.com/embed/MHhlAtw3Ces\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104c0d690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole on Model Selection\n",
    "YouTubeVideo(\"MHhlAtw3Ces\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.youtube.com/embed/RZmitKn220Q\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104c0d5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole on feature extraction and selection\n",
    "YouTubeVideo(\"RZmitKn220Q\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Reading_: DSFS Chapter 11. This covers really important ground in machine learning. Take a close look at the text, and be ready to return to this chapter after trying out some machine learning methods in subsequent lectures.\n",
    "\n",
    "---\n",
    "\n",
    "> _Exercises_: A few questions about machine learning. \n",
    "> * What do we mean by a 'feature' in a machine learning model?\n",
    "> * What is the main problem with overfitting?\n",
    "> * Explain the connection between the bias-variance trade-off and overfitting/underfitting.\n",
    "> * The `Luke is for leukemia` is a great example of why _accuracy_ is not a good measure in very unbalanced problems. You know about the incidents dataset we've been working with. Try to come up with a similar example based on the data we've been working with today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: A little bit of plotting\n",
    "\n",
    "There's a lot of rich geo-data in the crime dataset that we haven't explored yet. Recall that every incident in the database also has an associated GPS location. We're not going to dig deep regarding the geodata quite yet, but it's interesting to just start plotting a little bit.\n",
    "\n",
    "We'll be using the package [`geoplotlib`](https://github.com/andrea-cuttone/geoplotlib) to plot geographical data (it's like `matplotlib` but for geo-data). You can't install geoplotlib using `conda install` since it's not part of the Anaconda framework. But we _do_ want to install `geoplotlib` so that it integrates with Anaconda. The way to do that is to use another package installer called [`pip`](https://en.wikipedia.org/wiki/Pip_(package_manager)). Your system may have multiple versions of `pip` so, it's important that you use the one that comes with Anaconda. On my computer, that file is stored in `~/anaconda/bin`, so to install I simply type\n",
    "\n",
    "```\n",
    ">> ~/anaconda/bin/pip install pyglet\n",
    ">> ~/anaconda/bin/pip install geoplotlib\n",
    "```  \n",
    "That first line is there because you need to install a library called pyglet in order for `geoplotlib` to work.\n",
    "\n",
    "Now that we have the capabilities for plotting, we need to get data in the format that `geoplotlib` requires. Your put your geodata in a dictionary structured as follows.\n",
    "\n",
    "```\n",
    "geo_data_for_plotting = {\"lat\": [list-of-latitudes],\n",
    "                         \"lon\": [list of longitudes]}\n",
    "```\n",
    "\n",
    "Check out the [user guide](https://github.com/andrea-cuttone/geoplotlib/wiki/User-Guide) and [example gallary](https://github.com/andrea-cuttone/geoplotlib) to see some of the cool things you can do.\n",
    "\n",
    "**Oh and one last thing**: In order to plot inside your notebook, use `geoplotlib.inline()`\n",
    "\n",
    "> _Exercise_: Create a kernel density map plot of all crimes from the CSV file.\n",
    "> Here's my version\n",
    "\n",
    "> ![Histograms](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/kde_plot.png)\n",
    "\n",
    "> Note that there's very bright spot just south of south of market. Check out the map carefully and see if you can figure out what it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: K-nearest-neighbors. \n",
    "\n",
    "Hmm. I think the exercise are starting to extend beyond the 5 ECTS time-limit, so the difficult part of the K-nearest-neighbor exercise will be optional. You will have to view the video, read the chapter and answer a few general questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.youtube.com/embed/OE159z8kC-Y\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104c0d650>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole on K-nearest-neighbors\n",
    "YouTubeVideo(\"OE159z8kC-Y\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> _Reading_: DSFS Chapter 12. Pay special attention to the example on page 153-156.\n",
    "\n",
    "---\n",
    "> _Exercises_: K-nearest-neighbors\n",
    "> * How does K-nearest-neighbors work? Explain in your own words.\n",
    "> * Explain in your own words: What is the curse of dimensionality? Use figure 12-6 to as part of your explanation.\n",
    "> * **Optional, but recommended (because it's awesome)**: Adjust the example on pp 153 to work on actual crime data. \n",
    ">   - Since we know from the exercises above that focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE` tend to be concentrated in certain neighborhoods, let's focus on those crime types since they will make the most sense for the 2d version of KNN.\n",
    ">   - Begin by using `geoplotlib` to plot all incidents of all three crime types on a map using three different colors.\n",
    ">   - Now create a square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $25 \\times 25$ and $100 \\times 100$ points.\n",
    ">   - Create three maps. One, where you color each point according to its nearest neighbor, another where you color each point according to the 3 nearest neighbors, and finally one where you color according to the 5 nearest neighbors (the book has details on distances, voting, etc).\n",
    "\n",
    "Note that these maps would _actually be useful_ to the SFPD when they decide on patroling, special assignments, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
